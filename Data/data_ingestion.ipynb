{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "Data Sources: <br>\n",
    "<ul>\n",
    "    <li>SP500 Data: Yahoo! Finance</li>\n",
    "    <li>Federal Funds Data: FRED</li>\n",
    "    <li>Industrial Production Data: FRED</li>\n",
    "    <li>Value/Momentum Data: AQR</li>\n",
    "</ul>\n",
    "\n",
    "**Goal**: The purpose of this notebook is to curate one dataset that has each of the datum above as a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in individual data sources\n",
    "sp500 = pd.read_csv(\"./DataSrcs/sp500_data.csv\")\n",
    "fed   = pd.read_csv(\"./DataSrcs/FEDFUNDS.csv\")\n",
    "ipi   = pd.read_csv(\"./DataSrcs/INDPRO.csv\")\n",
    "vmd   = pd.read_csv(\"./DataSrcs/value_momentum_data.csv\")\n",
    "\n",
    "## Update - 12/4/2018, fetch FF data\n",
    "ff = pd.read_csv(\"./DataSrcs/ff_data.csv\")\n",
    "ff.Date = ff.Date.apply( lambda x: datetime.strptime(str(x), \"%Y%m\").date() )\n",
    "ffmom = pd.read_csv(\"./DataSrcs/ff_mom_data.csv\")\n",
    "ffmom.Date = ffmom.Date.apply( lambda x: datetime.strptime(str(x), \"%Y%m\").date() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_series = pd.Series( index = pd.RangeIndex(0, len(vmd.Date)) )\n",
    "for idx, date in enumerate(vmd.Date):\n",
    "    L = date.split(\"/\"); m = L[0]; d = L[1]; year = L[2]\n",
    "    \n",
    "    # check the year\n",
    "    if int(year) >= 72:\n",
    "        year = \"19\" + year\n",
    "    else:\n",
    "        year = \"20\" + year\n",
    "        \n",
    "    date_series[idx] = datetime(int(year), int(m), 1).date()\n",
    "    \n",
    "vmd.Date = date_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homogenizing\n",
    "\n",
    "Let's make all of the data homogenous - i.e. same data format to groupby. Also over the same time range. To that end, we'll look from February 1972 - September 2018 and we're going to remove each of the \"days\" once we convert the date columns to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(1972, 2, 1); end_date = datetime(2012, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [sp500, fed, ipi, vmd, ff, ffmom]\n",
    "for df in all_data:\n",
    "    dates = df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to datetime\n",
    "sp500.Date = pd.to_datetime(sp500.Date)\n",
    "fed.DATE   = pd.to_datetime(fed.DATE)\n",
    "ipi.DATE   = pd.to_datetime(ipi.DATE)\n",
    "vmd.Date   = pd.to_datetime(vmd.Date)\n",
    "ff.Date    = pd.to_datetime(ff.Date)\n",
    "ffmom.Date = pd.to_datetime(ffmom.Date)\n",
    "\n",
    "# Filter so that it's in between Feb72 - Sep18\n",
    "sp500  = sp500[ (start_date <= sp500.Date) & (sp500.Date <= end_date) ]\n",
    "fed    = fed[ (start_date <= fed.DATE) & (fed.DATE <= end_date) ]\n",
    "ipi    = ipi[ (start_date <= ipi.DATE) & (ipi.DATE <= end_date) ]\n",
    "vmd    = vmd[ (start_date <= vmd.Date) & (vmd.Date <= end_date) ]\n",
    "ff     = ff[ (start_date <= ff.Date) & (ff.Date <= end_date) ]\n",
    "ffmom  = ffmom[ (start_date <= ffmom.Date) & (ffmom.Date <= end_date) ]\n",
    "\n",
    "# Redefine with the new dataframes\n",
    "all_data = [sp500, fed, ipi, vmd, ff, ffmom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all individual dataframes\n",
    "dataset = pd.concat(all_data, axis=1)\n",
    "dataset.index = dataset.iloc[:,0]\n",
    "dataset = dataset.drop([\"Date\", \"DATE\", \"DATE\", \"Date\"], axis=1)\n",
    "dataset[\"VAL^US\"] = dataset[\"VAL^US\"].shift(-1)\n",
    "dataset[\"MOM^US\"] = dataset[\"MOM^US\"].shift(-1)\n",
    "\n",
    "# Drop unneeded columns\n",
    "dataset = dataset.drop( [\"Open\", \"High\", \"Low\", \"Close\", \"HML\", \"RMW\", \"CMA\", \"RF\"], axis=1 )\n",
    "#dataset[\"Mkt-RF\"] /= 100; dataset[\"SMB\"] /= 100; dataset.iloc[:,8] /= 100\n",
    "\n",
    "# Convert pcts to decimals, strings to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'na'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8b80669e656d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"VAL^US\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"na\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8b80669e656d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"VAL^US\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"na\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'na'"
     ]
    }
   ],
   "source": [
    "dataset[\"VAL^US\"].apply( lambda x: float(str(x)[:-1]) if str(x) != \"na\" else np.na )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv file for you all to just pull from the repo\n",
    "# It will easily create the data set as \"dataset_{}\" where {} is the date you ran this code\n",
    "td = str(datetime.today().date())\n",
    "dataset.to_csv( \"./all_dataset_{}.csv\".format(td, td) )\n",
    "\n",
    "## Build an \"AQR\" dataset\n",
    "aqr_dataset = ff_dataset = dataset.iloc[:,[0,1,2,3,4,5]]\n",
    "aqr_dataset.to_csv( \"./aqr_dataset_{}.csv\".format(td, td) )\n",
    "\n",
    "## Build an \"FF\" dataset\n",
    "ff_dataset  = dataset.iloc[:,[1,3,6,7,8]]\n",
    "ff_dataset.to_csv( \"./ff_dataset_{}.csv\".format(td, td) )\n",
    "\n",
    "# Put all in the correct directory\n",
    "os.system( \"mkdir '{}_Data'\".format(td) )\n",
    "os.system( \"mv 'aqr_dataset_{}.csv' 'ff_dataset_{}.csv' \\\n",
    "'all_dataset_{}.csv' './{}_Data'\".format(td, td, td, td) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
